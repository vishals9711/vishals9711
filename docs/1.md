Of course. Adopting a template-based approach is the superior strategy for consistency and maintenance. Using a custom script gives us the flexibility you're looking for.

Let's structure this as a formal project plan using an Agile framework. This will give us a clear rundown, a technical specification for each feature, and a roadmap for implementation.

***

### **Project Rundown: "Profile Dynamo"**

**Project Vision:** To create a fully automated, custom system that generates a dynamic and up-to-date GitHub profile README. The system will be powered by a custom script, orchestrated by a GitHub Actions workflow, ensuring complete control over the content and presentation.

**Core Components:**
1.  **`README.template.md`**: A static template file containing the overall layout, static text, and placeholders for dynamic content.
2.  **`update_profile.py`** (or `.js`): A custom script that acts as the engine. It fetches data from APIs, processes it, and populates the template.
3.  **GitHub Actions Workflow**: The orchestrator that runs on a schedule, executes the script, and commits the final `README.md`.
4.  **GitHub Secrets**: A secure store for API keys and tokens.

**Tech Stack:**
* **Orchestration**: GitHub Actions
* **Scripting Language**: Python (Recommended for its excellent `requests` and data handling libraries) or Node.js.
* **APIs**: GitHub GraphQL API (v4), Wakatime API (v1)

**Methodology:** We will break this project down into four main **Epics** (large features). Each Epic will contain specific **User Stories** with clear **Acceptance Criteria** and a **Technical Implementation Plan**.

***

### **Technical Documentation & Agile Plan**

### Epic 1: Foundational Setup & Workflow Orchestration

*This epic covers creating the skeleton of our project: the files, secrets, and the workflow that ties everything together.*

---

#### **User Story 1.1: Environment Setup**
**As a developer, I want to** set up the repository structure and secrets **so that** I have a secure and organized foundation for the automation script and workflow.

**Acceptance Criteria:**
1.  A `README.template.md` file exists in the root of the repository.
2.  A `scripts/` directory is created, ready for the automation script.
3.  Two secrets are created in the repository's settings (`Settings > Secrets and variables > Actions`):
    * `WAKATIME_API_KEY`: Your Wakatime API key.
    * `GH_TOKEN`: A GitHub Personal Access Token with `repo` and `user` scopes.

**Technical Implementation Plan:**
* **`README.template.md`**: This file will be a copy of your desired README layout. Dynamic content will be represented by unique placeholders, e.g., `{WAKATIME_STATS}`, `{GITHUB_LANGUAGES}`, `{CONTRIBUTION_STATS}`.
* **Secrets**: The `GH_TOKEN` is used to authenticate with the GitHub API to get higher rate limits and access private repo stats if needed. The `WAKATIME_API_KEY` is for the Wakatime API. They *must* be stored as secrets and passed to the script as environment variables.

---

#### **User Story 1.2: Workflow Trigger & Execution**
**As a developer, I want to** create a GitHub Actions workflow **so that** my custom script is automatically executed on a weekly basis and commits the result.

**Acceptance Criteria:**
1.  A workflow file exists at `.github/workflows/profile-update.yml`.
2.  The workflow is triggered every Sunday at 05:00 UTC.
3.  The workflow successfully checks out the repository code.
4.  The workflow sets up the chosen scripting environment (e.g., Python 3.10).
5.  The script at `scripts/update_profile.py` is executed.
6.  If `README.md` is changed, the workflow commits the file back to the `main` branch.

**Technical Implementation Plan:**
* **Trigger**: Use the `on: schedule:` cron syntax: `cron: '0 5 * * 0'`.
* **Job Steps**:
    1.  `actions/checkout@v4`: To access the repository files.
    2.  `actions/setup-python@v5`: To prepare the Python environment.
    3.  `Run pip install -r requirements.txt`: To install script dependencies like `requests`.
    4.  `Run python scripts/update_profile.py`: Execute the script, passing secrets as environment variables (`env:` block).
    5.  `stefanzweifel/git-auto-commit-action@v5`: An easy-to-use action that will detect changes to `README.md` and commit them.

***

### Epic 2: Data Aggregation Engine

*This epic focuses on the script's ability to fetch all the necessary raw data from external APIs.*

---

#### **User Story 2.1: Fetch GitHub Profile Stats**
**As a profile viewer, I want to** see accurate and current GitHub statistics **so that** I can understand the developer's recent activity and areas of focus.

**Acceptance Criteria:**
1.  The script authenticates with the GitHub API using the `GH_TOKEN`.
2.  The script successfully fetches total contributions, pull requests, and issues for the year.
3.  The script fetches data for the user's top 5 pinned repositories.
4.  The script fetches a breakdown of the top languages used across the user's repositories.

**Technical Implementation Plan:**
* **API Choice**: Use the **GitHub GraphQL API (v4)**. It's highly efficient, allowing us to fetch all the required data in a single network request.
* **Query Structure**: Construct a GraphQL query to retrieve `viewer` information. This will include nested queries for:
    * `contributionsCollection` for commit, PR, and issue counts.
    * `pinnedItems` for details on pinned repositories (name, description, URL, primary language).
    * `repositories` to iterate through and aggregate language usage statistics.
* **Implementation**: In the script, define the GraphQL query string. Use the `requests` library to send a `POST` request to `https://api.github.com/graphql` with the appropriate `Authorization` header (`Bearer <GH_TOKEN>`) and the JSON payload containing the query.

---

#### **User Story 2.2: Fetch Wakatime Coding Stats**
**As a profile viewer, I want to** see a summary of the developer's recent coding activity **so that** I know what technologies they are actively using.

**Acceptance Criteria:**
1.  The script authenticates with the Wakatime API using the `WAKATIME_API_KEY`.
2.  The script fetches the user's coding activity summary for the last 7 days.
3.  The fetched data includes time spent per language, editor, and operating system.

**Technical Implementation Plan:**
* **API Endpoint**: Make a `GET` request to the Wakatime summaries endpoint: `https://wakatime.com/api/v1/users/current/summaries`.
* **Parameters**: Pass the `range=last_7_days` query parameter.
* **Authentication**: The `WAKATIME_API_KEY` should be passed in the `Authorization` header.
* **Implementation**: Use the `requests` library to perform the GET request and parse the resulting JSON.

***

### Epic 3: Data Processing & Content Generation

*This epic is about transforming the raw JSON data into human-readable, formatted content.*

---

#### **User Story 3.1: Generate Visualizations and Formatted Text**
**As a developer, I want to** process the raw API data into formatted text and visualizations **so that** the final README is clear, engaging, and easy to read.

**Acceptance Criteria:**
1.  Wakatime's total coding time (in seconds) is converted into a "X hrs Y mins" string.
2.  A text-based bar chart for the top 5 Wakatime languages is generated.
3.  GitHub contribution numbers are formatted with commas for readability (e.g., 1,234).
4.  Pinned repositories are formatted into a clean list using Markdown.

**Technical Implementation Plan:**
* **Time Formatting**: Create a helper function `format_time(seconds)` that uses integer division and modulo arithmetic to calculate hours and minutes.
* **Text-based Bar Chart**: Create a function `generate_bar_chart(data, total)`. For each language, calculate its percentage of the total. Create a string of block characters (e.g., `‚ñà`) proportional to the percentage, followed by the language name and percentage text.
    * Example: `Python üêç   [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   18 hrs 30 mins (45.0%)`
* **Data to String Conversion**: The script will hold all generated content in variables, ready for injection.

***

### Epic 4: README Generation & Deployment

*The final epic: putting it all together to create the final artifact and deploy it.*

---

#### **User Story 4.1: Populate Template and Generate README**
**As a script, I want to** read the template file and inject the generated content **so that** a final, complete `README.md` is created.

**Acceptance Criteria:**
1.  The script successfully reads the entire content of `README.template.md`.
2.  All placeholders (e.g., `{WAKATIME_STATS}`) in the template are replaced with the corresponding generated content.
3.  A new `README.md` file is created (or overwritten) with the final content.
4.  The script handles cases where an API might fail, ensuring it doesn't break the README.

**Technical Implementation Plan:**
* **File I/O**: Use standard file handling in your chosen language to read `README.template.md`.
* **String Replacement**: Use the `.replace()` or f-string/template literal functionality to substitute placeholders with the variables containing your formatted data.
* **Error Handling**: Wrap API calls in `try...except` blocks. If an API fails, you can either fall back to placeholder text like "Could not retrieve stats" or leave the previous content untouched.
* **File Write**: Write the final, populated string to the `README.md` file.

### **Next Steps**

1.  **Sprint 1**: Focus on **Epic 1**. Set up the repository, create the template and secrets, and build the basic workflow file that can run a simple "Hello World" script.
2.  **Sprint 2**: Tackle **Epic 2**. Write the script logic to successfully connect to both APIs and print the raw JSON data.
3.  **Sprint 3**: Build out **Epic 3 & 4**. Implement the data formatting functions and the template injection logic.

This structured plan should give you a very clear path forward for building your custom profile generator.